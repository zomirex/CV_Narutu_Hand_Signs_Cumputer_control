import whisper
import sounddevice as sd
import numpy as np
import torch
from silero_vad import load_silero_vad, get_speech_timestamps

# ==================== ุชูุธูุงุช ====================
MODEL_SIZE = "small"  # ุง "medium" ุจุฑุง ุฏูุช ุจุดุชุฑ
SAMPLERATE = 16000  # โ๏ธ ุญุชูุงู 16000 ุจุงุดุฏ (32000 ุจุงุนุซ ุฎุทุง ูโุดูุฏ!)
CHUNK_DURATION = 1.0
MIN_SPEECH_DURATION = 0.3
OVERLAP = 0.3

# ==================== ุจุงุฑฺฏุฐุงุฑ ูุฏูโูุง ====================
print("ุฏุฑ ุญุงู ุจุงุฑฺฏุฐุงุฑ ูุฏูโูุง...")
vad_model = load_silero_vad()
whisper_model = whisper.load_model(MODEL_SIZE)
print(f"โ ูุฏู Whisper ({MODEL_SIZE}) ู VAD ุจุงุฑฺฏุฐุงุฑ ุดุฏูุฏ")

# ==================== ุถุจุท ู ูพุฑุฏุงุฒุด ููุดููุฏ ====================
buffer = np.array([], dtype=np.float32)
print("\n๐ค ุดุฑูุน ุถุจุท... (Ctrl+C ุจุฑุง ุชููู)")

try:
    while True:
        audio_chunk = sd.rec(
            int(CHUNK_DURATION * SAMPLERATE),
            samplerate=SAMPLERATE,  # โ 16000
            channels=1,
            dtype=np.float32
        )
        sd.wait()
        audio_chunk = audio_chunk.flatten()

        if len(buffer) > 0:
            audio_chunk = np.concatenate([buffer, audio_chunk])
        buffer = audio_chunk[-int(OVERLAP * SAMPLERATE):]

        # โ ุญุงูุง ุจุง 16000 ูุฑุชุฒ ฺฉุงุฑ ูโฺฉูุฏ
        speech_timestamps = get_speech_timestamps(
            audio_chunk,
            vad_model,
            sampling_rate=SAMPLERATE,  # โ 16000
            min_speech_duration_ms=int(MIN_SPEECH_DURATION * 1000),
            threshold=0.5
        )

        if speech_timestamps:
            print("\n๐ ฺฏูุชุงุฑ ุดูุงุณุง ุดุฏ...")
            speech_segments = [audio_chunk[ts['start']:ts['end']] for ts in speech_timestamps]
            full_speech = np.concatenate(speech_segments)
            full_speech = full_speech / (np.max(np.abs(full_speech)) + 1e-8)

            # โ ุจุฏูู ุชุนู ุฒุจุงู โ Whisper ุฎูุฏุด ูุงุฑุณ/ุงูฺฏูุณ ุฑุง ุชุดุฎุต ูโุฏูุฏ
            result = whisper_model.transcribe(
                full_speech,
                language='en',
                fp16=torch.cuda.is_available(),
                temperature=0.0,
                no_speech_threshold=0.4
            )

            text = result["text"].strip()
            if len(text) > 3:
                print(f"๐ฌ {text}")
            else:
                print("โ๏ธ ูุชู ุจุณุงุฑ ฺฉูุชุงู (ุงุญุชูุงูุงู ููุฒ)")

except KeyboardInterrupt:
    print("\nโน๏ธ ุถุจุท ูุชููู ุดุฏ")
except Exception as e:
    print(f"\nโ ุฎุทุง: {type(e).__name__} - {e}")
    print("๐ก ูฺฉุชู: ุงฺฏุฑ ุฎุทุง ูุฑุจูุท ุจู ูุฑุฎ ูููููโุจุฑุฏุงุฑ ุงุณุชุ ุญุชูุงู SAMPLERATE=16000 ุฑุง ุจุฑุฑุณ ฺฉูุฏ!")