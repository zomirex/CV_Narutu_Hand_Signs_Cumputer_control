import cv2
import mediapipe as mp
import numpy as np
import pyautogui
import time

# --- ØµØ¯Ø§ (ÙÙ‚Ø· ÙˆÛŒÙ†Ø¯ÙˆØ²) ---
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
from comtypes import CLSCTX_ALL
from ctypes import cast, POINTER

# --- MediaPipe ---
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµØ¯Ø§ ---
devices = AudioUtilities.GetSpeakers()
interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
volume = cast(interface, POINTER(IAudioEndpointVolume))
try:
    vol_percent = int(volume.GetMasterVolumeLevelScalar() * 100)
except:
    vol_percent = 50

# --- Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ ---
brightness = 100
prev_right_x = None
prev_left_x = None

# --- Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Alt+Tab Ù¾Ø§ÛŒØ¯Ø§Ø± ---
alt_held = False
prev_hand_x = None
last_tab_time = 0
TAB_COOLDOWN = 0.1
alt_mode_counter = 0
exit_counter = 0
CONFIRM_FRAMES = 3  # ØªØ¹Ø¯Ø§Ø¯ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ ØªØ£ÛŒÛŒØ¯
DISTANCE_THRESHOLD = 0.06  # Ø¢Ø³ØªØ§Ù†Ù‡ ÙØ§ØµÙ„Ù‡ Ø´Ø³Øª Ùˆ Ø§Ø´Ø§Ø±Ù‡

# --- ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ---
def euclidean_distance(p1, p2):
    return np.sqrt((p1.x - p2.x)**2 + (p1.y - p2.y)**2)

def other_fingers_open(landmarks):
    def is_finger_open(tip, dip):
        return landmarks.landmark[tip].y < landmarks.landmark[dip].y
    return is_finger_open(12, 10) and is_finger_open(16, 14) and is_finger_open(20, 18)

def is_fist(hand_landmarks):
    tips = [8, 12, 16, 20]
    dips = [6, 10, 14, 18]
    for tip, dip in zip(tips, dips):
        if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[dip].y:
            return False
    return True

# --- Ø´Ø±ÙˆØ¹ ---
cap = cv2.VideoCapture(0)
print("Ø³ÛŒØ³ØªÙ… Ú©Ù†ØªØ±Ù„ Ø¯Ø³Øª ÙØ¹Ø§Ù„ Ø´Ø¯!")
print("- Ø¯Ø³Øª Ø±Ø§Ø³Øª Ù…Ø´Øª â†’ Ù†ÙˆØ±")
print("- Ø¯Ø³Øª Ú†Ù¾ Ù…Ø´Øª â†’ ØµØ¯Ø§")
print("- Ø´Ø³Øª+Ø§Ø´Ø§Ø±Ù‡ Ú†Ø³Ø¨ÛŒØ¯Ù‡ + Ø¨Ù‚ÛŒÙ‡ Ø¨Ø§Ø² â†’ Alt+Tab Ù¾ÛŒØ´Ø±ÙØªÙ‡")

while cap.isOpened():
    success, image = cap.read()
    if not success:
        continue

    image = cv2.flip(image, 1)
    h, w, _ = image.shape
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb_image)

    current_hand_x = None

    if results.multi_hand_landmarks:
        hand_landmarks = results.multi_hand_landmarks[0]
        mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
        current_hand_x = hand_landmarks.landmark[0].x * w

        # --- ØªØ´Ø®ÛŒØµ Alt+Tab Ø¨Ø§ ØªØ£ÛŒÛŒØ¯ Ú†Ù†Ø¯ ÙØ±ÛŒÙ…ÛŒ ---
        thumb_tip = hand_landmarks.landmark[4]
        index_tip = hand_landmarks.landmark[8]
        distance = euclidean_distance(thumb_tip, index_tip)
        others_open = other_fingers_open(hand_landmarks)

        if distance < DISTANCE_THRESHOLD and others_open:
            alt_mode_counter += 1
            exit_counter = 0
            if alt_mode_counter >= CONFIRM_FRAMES and not alt_held:
                print("ğŸ¡ Alt ÙØ´Ø±Ø¯Ù‡ Ø´Ø¯ (Ù…Ù†ÙˆÛŒ Ø³ÙˆØ¦ÛŒÚ† Ø¨Ø§Ø² Ø´Ø¯)")
                pyautogui.keyDown('alt')
                pyautogui.press('tab')
                alt_held = True
                prev_hand_x = current_hand_x
        elif alt_held:
            exit_counter += 1
            alt_mode_counter = 0
            if exit_counter >= CONFIRM_FRAMES:
                print("âœ“ Alt Ø±Ù‡Ø§ Ø´Ø¯ (Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯)")
                pyautogui.keyUp('alt')
                alt_held = False
                prev_hand_x = None
                exit_counter = 0
        else:
            alt_mode_counter = 0
            exit_counter = 0

        # --- Ú©Ù†ØªØ±Ù„ Ù†ÙˆØ± Ùˆ ØµØ¯Ø§ (ÙÙ‚Ø· Ø®Ø§Ø±Ø¬ Ø§Ø² Ø­Ø§Ù„Øª Alt+Tab) ---
        if not alt_held:
            cx = hand_landmarks.landmark[0].x
            label = "Right" if cx < 0.5 else "Left"
            cx_px = cx * w

            if label == "Right" and is_fist(hand_landmarks):
                if prev_right_x is not None:
                    delta = (cx_px - prev_right_x) / w
                    brightness = np.clip(brightness + delta * 500, 0, 100)
                prev_right_x = cx_px

            elif label == "Left" and is_fist(hand_landmarks):
                if prev_left_x is not None:
                    delta = (cx_px - prev_left_x) / w
                    new_vol = vol_percent + delta * 250
                    vol_percent = np.clip(new_vol, 0, 100)
                    volume.SetMasterVolumeLevelScalar(vol_percent / 100.0, None)
                prev_left_x = cx_px

            else:
                if label == "Right":
                    prev_right_x = cx_px
                else:
                    prev_left_x = cx_px

        # --- Ø­Ø±Ú©Øª Ø¨ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø­Ø§Ù„Øª Alt+Tab ---
        else:
            if prev_hand_x is not None:
                dx = (current_hand_x - prev_hand_x) / w
                current_time = time.time()
                if abs(dx) > 0.02 and (current_time - last_tab_time) > TAB_COOLDOWN:
                    if dx > 0:
                        print("â†’")
                        pyautogui.press('left')
                    else:
                        print("â†")
                        pyautogui.press('right')
                    last_tab_time = current_time
            prev_hand_x = current_hand_x

    else:
        # Ø¯Ø³Øª Ù†ÛŒØ³Øª â†’ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø±ÛŒØ³Øª Ø´ÙˆØ¯
        if alt_held:
            pyautogui.keyUp('alt')
            alt_held = False
        prev_right_x = None
        prev_left_x = None
        prev_hand_x = None
        alt_mode_counter = 0
        exit_counter = 0

    # --- Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ± ---
    overlay = image.copy()
    alpha = 1.0 - brightness / 100.0
    alpha = np.clip(alpha, 0, 1)
    image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)

    # --- Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª ---
    cv2.putText(image, f"Brightness: {int(brightness)}%", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    cv2.putText(image, f"Volume: {int(vol_percent)}%", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    status = "Alt+Tab Mode" if alt_held else "Normal"
    cv2.putText(image, status, (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

    cv2.imshow("Hand Control - Advanced", image)

    if cv2.waitKey(5) & 0xFF == 27:  # ESC
        break

# --- Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ---
if alt_held:
    pyautogui.keyUp('alt')
cap.release()
cv2.destroyAllWindows()
print("Ø³ÛŒØ³ØªÙ… Ø¨Ø³ØªÙ‡ Ø´Ø¯.")